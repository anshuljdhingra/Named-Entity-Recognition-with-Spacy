{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Website List File Path : C:\\Users\\gaurav.anand\\OneDrive - NIIT Technologies 1\\Automation COC\\SVN\\WebScraping\\WebScraping\\Websites-Single.xlsx\n",
      "Output result file is : C:\\Users\\gaurav.anand\\Downloads\\Web Scraping AI\\Website_Extraction_results.csv\n",
      "Output Content exyraction path is : C:\\Users\\gaurav.anand\\Downloads\\Web Scraping AI\\extracts\n",
      "Result File deleted : C:\\Users\\gaurav.anand\\Downloads\\Web Scraping AI\\Website_Extraction_results.csv\n",
      "Number of Web Sites : 2\n",
      "Http Index in URL www.4-cwealth.com is : -1\n",
      "Prefixing URL with Http\n",
      "Extracting Page URL  : http://www.4-cwealth.com\n",
      "Is Subsite : False\n",
      "Opening URL now : http://www.4-cwealth.com\n",
      "Successfully Opened Url\n",
      "===>>>>>>>    =====<<<<<<\n",
      "\n",
      " Subsite : /contact_us is relevant for scraping for keyword : contact\n",
      "===>>>>>>>    =====<<<<<<\n",
      "Checking for Frames\n",
      "   \n",
      "   \n",
      "====================\n",
      " Printing Webtext  \n",
      "====================\n",
      "Writing file for http://www.4-cwealth.com to Disk \n",
      "Web Site Content for http://www.4-cwealth.com has been written to Disk File\n",
      "Http Index in URL http://www.4-cwealth.com/contact_us is : 0\n",
      "Extracting Page URL  : http://www.4-cwealth.com/contact_us\n",
      "Is Subsite : True\n",
      "Opening URL now : http://www.4-cwealth.com/contact_us\n",
      "Successfully Opened Url\n",
      "Checking for Frames\n",
      "   \n",
      "   \n",
      "====================\n",
      " Printing Webtext  \n",
      "====================\n",
      "Writing file for http://www.4-cwealth.com/contact_us to Disk \n",
      "Web Site Content for http://www.4-cwealth.com/contact_us has been written to Disk File\n",
      "['http://www.4-cwealth.com/contact_us']\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "#import tldextract\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import json\n",
    "#from urlparse import urljoin\n",
    "\n",
    "\n",
    "#=== Read Excel Sheet =========\n",
    "import xlrd\n",
    "\n",
    "#=== Write to CSV file ===\n",
    "import csv\n",
    "\n",
    "#==Os library baig used to delete a file\n",
    "import os\n",
    "\n",
    "urlResultSheetPath=\"\" #=r\"C:\\Users\\gaurav.anand\\Downloads\\Web Scraping AI\\Website_Extraction_results.csv\"\n",
    "urlSheetPath=\"\"\n",
    "contentFilePath=\"\"\n",
    "data=\"\"\n",
    "\n",
    "def read_configuration():\n",
    "    global urlResultSheetPath\n",
    "    global urlSheetPath\n",
    "    global contentFilePath\n",
    "    global data\n",
    "    with open('Configuration.json') as json_data_file:\n",
    "        data = json.load(json_data_file)        \n",
    "        \n",
    "    print(\"Input Website List File Path : \"+ data[\"input\"][\"Input_WebSite_list\"])\n",
    "    #urlSheetPath=data[\"input\"][\"Input_WebSite_list\"]\n",
    "        \n",
    "    print(\"Output result file is : \"+data[\"output\"][\"Extration_Result_File_Path\"])\n",
    "    #urlResultSheetPath=data[\"output\"][\"Extration_Result_File_Path\"]\n",
    "        \n",
    "        \n",
    "    print(\"Output Content exyraction path is : \"+data[\"output\"][\"Extrated_Content_File_Path\"])\n",
    "    #contentFilePath=data[\"output\"][\"Extrated_Content_File_Path\"]  \n",
    "\n",
    "def delete_extraction_result_File():\n",
    "    #global urlResultSheetPath\n",
    "    \n",
    "    urlResultSheetPath=data[\"output\"][\"Extration_Result_File_Path\"]\n",
    "    if os.path.exists(urlResultSheetPath):\n",
    "        os.remove(urlResultSheetPath)\n",
    "        print(\"Result File deleted : \" + urlResultSheetPath)\n",
    "    \n",
    "#zero based Index and Column\n",
    "def read_excel_sheet(sheetIndex, rowNum, colNum):\n",
    "    #global urlSheetPath\n",
    "    #urlSheetPath=r\"C:\\Users\\gaurav.anand\\OneDrive - NIIT Technologies 1\\Automation COC\\SVN\\WebScraping\\WebScraping\\Websites for Charles.xlsx\"\n",
    "    #urlSheetPath=r\"C:\\Users\\gaurav.anand\\OneDrive - NIIT Technologies 1\\Automation COC\\SVN\\WebScraping\\WebScraping\\Websites-Single.xlsx\"\n",
    "    urlSheetPath=data[\"input\"][\"Input_WebSite_list\"]\n",
    "    Web_Sites=[] #Holds thr tickets\n",
    "\n",
    "    \n",
    "    # To open Workbook \n",
    "    wb = xlrd.open_workbook(urlSheetPath) \n",
    "    sheet = wb.sheet_by_index(sheetIndex) \n",
    "\n",
    "    print(\"Number of Web Sites :\",sheet.nrows)\n",
    "\n",
    "    for i in range(rowNum,sheet.nrows):\n",
    "        Web_Sites.append(sheet.cell_value(i, colNum))\n",
    "       # print(sheet.cell_value(i, 0),' ',sheet.cell_value(i, 3))\n",
    "    \n",
    "    return Web_Sites\n",
    "\n",
    "def Addto_extrction_result_sheet(url,status,parentDomainUrl,error=None):\n",
    "    \n",
    "    mydata=[]\n",
    "    #global urlResultSheetPath\n",
    "    urlResultSheetPath=data[\"output\"][\"Extration_Result_File_Path\"]    \n",
    "    myFile = open(urlResultSheetPath, 'a') \n",
    "    with myFile:\n",
    "        writer = csv.writer(myFile,delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow([parentDomainUrl,url,status,error])\n",
    "        \n",
    "    \n",
    "\n",
    "#Removes all the special charachers in string/Website URL\n",
    "def strip_url(url):\n",
    "    return re.sub('\\W+','', url)\n",
    "    \n",
    "def write_Website_text(webText,url):\n",
    "    global contentFilePath\n",
    "    print(\"Writing file for \"+ url + \" to Disk \")\n",
    "    filename= strip_url(url)\n",
    "    #contentFilePath=r\"C:\\Users\\gaurav.anand\\OneDrive - NIIT Technologies 1\\Automation COC\\SVN\\WebScraping\\WebScraping\"\n",
    "    contentFilePath=data[\"output\"][\"Extrated_Content_File_Path\"]\n",
    "    contentFilePath+=\"\\\\\"+filename +\".txt\" \n",
    "    \n",
    "    #webSiteTextFile = open(contentFilePath,\"w\",encoding=\"utf-8\")\n",
    "    webSiteTextFile = open(contentFilePath,\"w\",encoding=\"utf-8\")\n",
    "    webSiteTextFile.write(webText)  \n",
    "    print(\"Web Site Content for \" + url +\" has been written to Disk File\")\n",
    "    webSiteTextFile.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Hint List of Sublinks\n",
    "hintUrlLinks=[]\n",
    "#hintUrlLinks.append(\"about us\")\n",
    "#hintUrlLinks.append(\"about-us\")\n",
    "hintUrlLinks.append(\"about\")\n",
    "hintUrlLinks.append(\"team\")\n",
    "hintUrlLinks.append(\"leadership\")\n",
    "hintUrlLinks.append(\"contact\")\n",
    "hintUrlLinks.append(\"people\")\n",
    "hintUrlLinks.append(\"story\")\n",
    "hintUrlLinks.append(\"professional\")\n",
    "#hintUrlLinks.append(\"our resources\")\n",
    "hintUrlLinks.append(\"biography\")\n",
    "hintUrlLinks.append(\"adviser\")\n",
    "\n",
    "\n",
    "#List of Website Sublinks\n",
    "websiteSubLinks=[]\n",
    "\n",
    "#Checks if webste is relevant for content scraping\n",
    "def isRelevantSubsite(subsiteUrl, hrefText):\n",
    "    isRelevantSubsite=None\n",
    "    \n",
    "    if(subsiteUrl.endswith(\".pdf\")):\n",
    "        print(\"URL Excluded : \"+ subsiteUrl)\n",
    "        \n",
    "    else:\n",
    "        matches = (x for x in hintUrlLinks if x.lower() in hrefText.lower() or x.lower() in subsiteUrl.lower())\n",
    "        \n",
    "        for x in matches:\n",
    "            isRelevantSubsite=True\n",
    "            print(\"===>>>>>>>    =====<<<<<<\")\n",
    "            print(\"\\n\" +\" Subsite : \" +subsiteUrl +\" is relevant for scraping for keyword : \" +  str(x))\n",
    "            print(\"===>>>>>>>    =====<<<<<<\")\n",
    "    \n",
    "    return isRelevantSubsite\n",
    "\n",
    "def addRelevantSubsitetoList(domainUrl,subsiteURL):\n",
    "    \n",
    "    global  websiteSubLinks\n",
    "    \n",
    "    #print('Pre --->Subsite being Added to List for extraction : '+ subsiteURL)\n",
    "    if  subsiteURL.find(\"http\")==-1:\n",
    "        if subsiteURL[0]!=r\"/\":\n",
    "            subsiteURL=\"/\"+subsiteURL\n",
    "        subsiteURL=domainUrl+subsiteURL\n",
    "    subsiteURL=subsiteURL.replace(\" \",\"\")\n",
    "   \n",
    "    #print('-->Subsite being Added to List for extraction : '+ subsiteURL)\n",
    "    if subsiteURL not in websiteSubLinks:\n",
    "        websiteSubLinks.append(subsiteURL)\n",
    "        #print('Subsite Added to List for extraction : '+ subsiteURL)\n",
    "    \n",
    "def remove_NewLine_Characters_fromWebText(webText):\n",
    "    webText=webText.replace('\\n',' ').replace('\\r','')\n",
    "    return webText\n",
    "    \n",
    "def is_ExcludeLine(lineString):\n",
    "    isBlackListed=False\n",
    "    #print(\" =>>> Line Strinig before Black Listing : \" + lineString)\n",
    "    blacklist = [\".html\",\"Â©\",\"facebook\",\"youtube\",\"twitter\",\"instagram\",\"linkedin\"]\n",
    "    for obj in blacklist:\n",
    "       \n",
    "        if obj in lineString:\n",
    "            #print(\"Black List Object : \" + obj)\n",
    "            isBlackListed=True\n",
    "                \n",
    "    return isBlackListed\n",
    "\n",
    "def make_string_replacements(webText):\n",
    "    # Python program to extract emails From \n",
    "    # the String By Regular Expression. \n",
    "\n",
    "\n",
    "    # \\S matches any non-whitespace character \n",
    "    # @ for as in the Email \n",
    "    # + for Repeats a character one or more times \n",
    "    lst = re.findall('\\S+@\\S+', webText)\n",
    "\n",
    "    # Printing of List \n",
    "    print(lst) \n",
    "    emailExtlist = [\".com\"]\n",
    "    for email in lst:\n",
    "        index=webText.index(email)\n",
    "        print(\"Email \" + email + \" found at index\" + str(index))\n",
    "        if index>-1:\n",
    "            emailExtIndex=email.index(\".com\")\n",
    "            emailNew=email\n",
    "            if emailExtIndex>-1:\n",
    "                \n",
    "                emailNew=emailNew.replace(\".com\",\".com \",1)\n",
    "                print(\"Replacing \"+ \"emailExtlist[0]\"+ \" with \" + emailNew)\n",
    "            webText=webText.replace(email,emailNew,1)\n",
    "            \n",
    "            print(\"inside email replacement loop\")\n",
    "\n",
    "\n",
    "        #print(\"The new string is : \"+ webText)\n",
    "    return webText\n",
    "\n",
    "\n",
    "def invoke_url(url):\n",
    "       \n",
    "        hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "        \n",
    "        print(\"Opening URL now : \" + url)\n",
    "        \n",
    "        \n",
    "        #proxy_support = urllib.request.ProxyHandler({\"http\": \"http://gaurav.anand:steelrigminton@28@172.18.50.192:80\",  \"https\": \"http://gaurav.anand:steelrigminton@28@172.18.50.192:80\"})\n",
    "        #opener = urllib.request.build_opener(proxy_support)\n",
    "\n",
    "        #urllib.request.install_opener(opener)\n",
    "\n",
    "        \n",
    "        \n",
    "        req=urllib.request.Request(url,headers=hdr)\n",
    "        #thePage=urllib.request.urlopen(url)\n",
    "        #thePage=urllib.request.urlopen(url, data=bytes(json.dumps(hdr), encoding=\"utf-8\"))\n",
    "        thePage=urllib.request.urlopen(req)\n",
    "        \n",
    "        print('Successfully Opened Url')\n",
    "        return thePage\n",
    "\n",
    "def extract_content_from_frames(soup,parentDomainUrl,isSubsite):\n",
    "    \n",
    "    webText=\"\"\n",
    "    print(\"Checking for Frames\")\n",
    "    \n",
    "    \n",
    "    for frame in soup.select(\"frameset frame\"):\n",
    "        print(\"Identified Frame.Attempting to joing Frame Url\")\n",
    "        \n",
    "        \n",
    "        httpIndex= parentDomainUrl.find(\"http\")\n",
    "        if httpIndex== -1:\n",
    "            parentDomainUrl=\"http://\"+parentDomainUrl\n",
    "            \n",
    "        print(\"Parent Url : \" + parentDomainUrl)\n",
    "        frame_url = urllib.parse.urljoin(parentDomainUrl, frame[\"src\"])\n",
    "        print(\"Joined Frame url : \" + frame_url)\n",
    "        #frame_url=frame[\"src\"]\n",
    "\n",
    "        response=invoke_url(frame_url)\n",
    "        frame_soup = BeautifulSoup(response, 'html.parser') \n",
    "        \n",
    "        for element in frame_soup.findAll():\n",
    "\n",
    "            print(\"Element Text is : \" + element.text)\n",
    "            webText +=element.text\n",
    "        \n",
    "    #print(webText)    \n",
    "    return webText\n",
    "    \n",
    "\n",
    "    \n",
    "def extract_website_content(url,parentDomainUrl,isSubsite=False):\n",
    "    if not isSubsite:\n",
    "        websiteSubLinks.clear()\n",
    "   \n",
    "    httpIndex= url.find(\"http\")\n",
    "    print(\"Http Index in URL \" + url + \" is : \" + str(httpIndex))\n",
    "    if httpIndex== -1:\n",
    "        print(\"Prefixing URL with Http\")\n",
    "        url=\"http://\"+url\n",
    "    \n",
    "        \n",
    "    print(\"Extracting Page URL  : \" + url) \n",
    "    print(\"Is Subsite : \" + str(isSubsite)) \n",
    "    \n",
    "    try:\n",
    "        \n",
    "        \n",
    "        thePage=invoke_url(url)\n",
    "        \n",
    "        soup=BeautifulSoup(thePage,\"html.parser\")\n",
    "        #soup=BeautifulSoup(thePage,\"lxml\")\n",
    "        \n",
    "        #print(soup)\n",
    "\n",
    "        #blacklist = [\"script\", \"style\",\"img\",\"link\" ]\n",
    "        blacklist = [\"script\", \"style\",\"img\" ]\n",
    "        [s.extract() for s in soup(blacklist)] # remove tags in Blacklist\n",
    "\n",
    "        webText=\"\"\n",
    "\n",
    "        \n",
    "        \n",
    "        for link in soup.findAll():\n",
    "            #print(link.get('href'))\n",
    "            #print(link.text)\n",
    "\n",
    "            #webText +=link.name\n",
    "            #webText+=\"\\n\"\n",
    "            if not is_ExcludeLine(link.text):\n",
    "                webText +=link.text\n",
    "                        \n",
    "            \n",
    "            if link.name==\"a\": # add Leadership/Contect list check\n",
    "                webText +=\" : \"\n",
    "                #webText+=str(link.get('href'))\n",
    "                href=str(link.get('href'))\n",
    "                if not is_ExcludeLine(href): #href.find(\".html\")==-1: \n",
    "                    webText+=href\n",
    "\n",
    "                if not isSubsite and isRelevantSubsite(str(link.get('href')),link.text):\n",
    "                      addRelevantSubsitetoList(url,str(link.get('href')))\n",
    "\n",
    "            \n",
    "            webText+=\"\\n\"\n",
    "            #webText +=\"=======\"\n",
    "            #webText+=\"\\n\"\n",
    "\n",
    "        #Adding content from frames\n",
    "        webText+=extract_content_from_frames(soup,parentDomainUrl,isSubsite)\n",
    "        \n",
    "        print (\"   \")\n",
    "        print (\"   \")\n",
    "        print (\"====================\")\n",
    "        print (\" Printing Webtext  \")\n",
    "        print (\"====================\")\n",
    "        #print (webText)\n",
    "        \n",
    "        #print(\"begin-- adding Email surfix and prefix with space\")\n",
    "        #webText=make_string_replacements(webText) # prefix and surfix an email id with space\n",
    "        #print(\"End-- adding Email surfix and prefix with space\")\n",
    "        write_Website_text(webText,url)\n",
    "        Addto_extrction_result_sheet(url,\"Success\",parentDomainUrl)\n",
    "\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Error in last operation : ', str(e) )\n",
    "        \n",
    "        Addto_extrction_result_sheet(url,\"Fail\",parentDomainUrl,str(e))\n",
    "        pass\n",
    "        \n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "def execute_content_extraction_engine():\n",
    "   \n",
    "    global urlArr\n",
    "    global websiteSubLinks\n",
    "    \n",
    "    url_list=[]\n",
    "    \n",
    "    read_configuration()\n",
    "    \n",
    "    #Delete Extraction Result Sheet\n",
    "    delete_extraction_result_File()\n",
    "    \n",
    "    #Read all 80K domain names from Excel Sheet\n",
    "    url_list=read_excel_sheet(0,1,0)\n",
    "    \n",
    "    #Extract Contant for Main Site\n",
    "    for urlSite in url_list:\n",
    "        extract_website_content(urlSite,urlSite,False)\n",
    "        #Extract Contant for Subsite\n",
    "        for urlSubSite in websiteSubLinks:\n",
    "            extract_website_content(urlSubSite,urlSite,True)\n",
    "    \n",
    "    \n",
    "    print(websiteSubLinks)\n",
    "\n",
    "execute_content_extraction_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
